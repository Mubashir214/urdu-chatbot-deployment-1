# app.py - COMPLETE WORKING VERSION
import streamlit as st
import torch
import torch.nn as nn
import json
import re
import math
import time
import random
import os

# Page configuration
st.set_page_config(
    page_title="Ø§Ø±Ø¯Ùˆ Ú†ÛŒÙ¹ Ø¨ÙˆÙ¹ - Urdu Chatbot",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Custom CSS for RTL support
st.markdown("""
<style>
    .urdu-text {
        font-family: 'Segoe UI', 'Arial', sans-serif;
        font-size: 18px;
        direction: rtl;
        text-align: right;
        line-height: 1.8;
    }
    .user-message {
        background-color: #e3f2fd;
        padding: 12px;
        border-radius: 15px;
        margin: 8px 0;
        border: 1px solid #90caf9;
        text-align: right;
    }
    .bot-message {
        background-color: #f3e5f5;
        padding: 12px;
        border-radius: 15px;
        margin: 8px 0;
        border: 1px solid #ce93d8;
        text-align: right;
    }
    .stTextInput > div > div > input {
        text-align: right;
        font-family: 'Segoe UI', 'Arial';
        font-size: 16px;
    }
    .success-box {
        background-color: #d4edda;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
        border: 1px solid #c3e6cb;
    }
    .warning-box {
        background-color: #fff3cd;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
        border: 1px solid #ffeaa7;
    }
    .error-box {
        background-color: #f8d7da;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
        border: 1px solid #f5c6cb;
    }
</style>
""", unsafe_allow_html=True)

# Simple Transformer Model (Compatible with your trained model)
class SimpleTransformer(nn.Module):
    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=2, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        
        # Embedding layers
        self.encoder_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.decoder_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        
        # Positional encoding
        self.pos_encoding = self.create_pos_encoding(5000, d_model)
        
        # Transformer layers
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
        
        # Output layer
        self.output_layer = nn.Linear(d_model, vocab_size)
        
    def create_pos_encoding(self, max_len, d_model):
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        return pe.unsqueeze(0)  # (1, max_len, d_model)
    
    def make_src_mask(self, src):
        return (src != 0)
    
    def make_tgt_mask(self, tgt_len, device):
        mask = torch.triu(torch.ones(tgt_len, tgt_len) * float('-inf'), diagonal=1)
        return mask.to(device)
    
    def forward(self, src, tgt):
        # Source processing
        src_mask = self.make_src_mask(src)
        src_embedded = self.encoder_embedding(src) * math.sqrt(self.d_model)
        src_embedded = src_embedded + self.pos_encoding[:, :src.size(1), :].to(src.device)
        
        memory = self.encoder(src_embedded, src_key_padding_mask=~src_mask)
        
        # Target processing
        tgt_mask = self.make_tgt_mask(tgt.size(1), tgt.device)
        tgt_embedded = self.decoder_embedding(tgt) * math.sqrt(self.d_model)
        tgt_embedded = tgt_embedded + self.pos_encoding[:, :tgt.size(1), :].to(tgt.device)
        
        output = self.decoder(tgt_embedded, memory, tgt_mask=tgt_mask, 
                            tgt_key_padding_mask=~self.make_src_mask(tgt),
                            memory_key_padding_mask=~src_mask)
        
        return self.output_layer(output)

class UrduChatbot:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.stoi = {}
        self.itos = {}
        self.vocab_size = 10000  # Default, will be updated from tokenizer
        self.max_len = 50
        self.model_loaded = False
        self.load_model()

    def load_model(self):
        """Load the trained model and tokenizer"""
        try:
            st.info("ğŸ”„ Ù…Ø§ÚˆÙ„ Ù„ÙˆÚˆ ÛÙˆ Ø±ÛØ§ ÛÛ’...")
            
            # Check if files exist
            if not os.path.exists('urdu_tokenizer.json'):
                st.error("âŒ urdu_tokenizer.json ÙØ§Ø¦Ù„ Ù†ÛÛŒÚº Ù…Ù„ÛŒ")
                return False
                
            if not os.path.exists('best_urdu_chatbot.pth'):
                st.error("âŒ best_urdu_chatbot.pth ÙØ§Ø¦Ù„ Ù†ÛÛŒÚº Ù…Ù„ÛŒ")
                return False
            
            # Load tokenizer
            with open('urdu_tokenizer.json', 'r', encoding='utf-8') as f:
                tokenizer_config = json.load(f)
                self.stoi = tokenizer_config['word_index']
                self.vocab_size = tokenizer_config.get('vocab_size', len(self.stoi) + 4)
                self.itos = {v: k for k, v in self.stoi.items()}
            
            st.success(f"âœ… Ù„ØºØª Ù„ÙˆÚˆ ÛÙˆ Ú¯Ø¦ÛŒ: {len(self.stoi)} Ø§Ù„ÙØ§Ø¸")
            
            # Initialize model
            self.model = SimpleTransformer(
                vocab_size=self.vocab_size,
                d_model=256,
                nhead=8,
                num_layers=2
            ).to(self.device)
            
            # Load model weights
            checkpoint = torch.load('best_urdu_chatbot.pth', map_location=self.device)
            
            if isinstance(checkpoint, dict):
                if 'model_state_dict' in checkpoint:
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    # Try loading the state dict directly
                    self.model.load_state_dict(checkpoint)
            else:
                # If it's directly the state dict
                self.model.load_state_dict(checkpoint)
            
            self.model.eval()
            self.model_loaded = True
            st.success("âœ… Ù…Ø§ÚˆÙ„ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ø³Û’ Ù„ÙˆÚˆ ÛÙˆ Ú¯ÛŒØ§!")
            return True
            
        except Exception as e:
            st.error(f"âŒ Ù…Ø§ÚˆÙ„ Ù„ÙˆÚˆ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ø®Ø±Ø§Ø¨ÛŒ: {str(e)}")
            import traceback
            st.error(f"ØªÙØµÛŒÙ„ÛŒ Ø®Ø±Ø§Ø¨ÛŒ: {traceback.format_exc()}")
            return False

    def normalize_urdu_text(self, text):
        """Normalize Urdu text"""
        if not isinstance(text, str):
            return ""
        # Remove diacritics and normalize characters
        text = re.sub(r'[\u0610-\u061A\u064B-\u065F\u06D6-\u06ED]', '', text)
        text = text.replace('\u0640', '')  # Remove Tatweel
        text = re.sub('[\u0622\u0623\u0625]', 'Ø§', text)  # Normalize Alef
        text = re.sub('[\u064A\u06CC]', 'ÛŒ', text)  # Normalize Yeh
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def encode_text(self, text):
        """Encode text to token indices"""
        try:
            text = self.normalize_urdu_text(text)
            if not text:
                return torch.tensor([[2, 3] + [0] * (self.max_len-2)], dtype=torch.long)
            
            # Simple tokenization (split by spaces)
            tokens = text.split()
            
            # Convert tokens to IDs
            ids = []
            for token in tokens:
                if token in self.stoi:
                    ids.append(self.stoi[token])
                else:
                    ids.append(self.stoi.get('<OOV>', 1))  # Use OOV token
            
            # Add SOS and EOS
            sos_token = self.stoi.get('<sos>', 2)
            eos_token = self.stoi.get('<eos>', 3)
            ids = [sos_token] + ids + [eos_token]
            
            # Truncate or pad
            if len(ids) > self.max_len:
                ids = ids[:self.max_len]
                ids[-1] = eos_token
            else:
                ids.extend([0] * (self.max_len - len(ids)))
                
            return torch.tensor([ids], dtype=torch.long)
            
        except Exception as e:
            st.error(f"Encoding error: {e}")
            # Return basic SOS-EOS sequence
            return torch.tensor([[2, 3] + [0] * (self.max_len-2)], dtype=torch.long)

    def decode_text(self, ids):
        """Convert token indices back to text"""
        try:
            tokens = []
            for id_val in ids:
                if id_val == 0:  # PAD token
                    continue
                if id_val == self.stoi.get('<sos>', 2):  # SOS token
                    continue
                if id_val == self.stoi.get('<eos>', 3):  # EOS token
                    break
                token = self.itos.get(id_val, '<UNK>')
                tokens.append(token)
            return ' '.join(tokens) if tokens else "Ù…Ø¬Ú¾Û’ Ø³Ù…Ø¬Ú¾ Ù†ÛÛŒÚº Ø¢ÛŒØ§Û”"
        except Exception as e:
            return "Ø¬ÙˆØ§Ø¨ Ø¬Ù†Ø±ÛŒÙ¹ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ù…Ø³Ø¦Ù„Û ÛÙˆØ§Û”"

    def generate_response(self, user_input):
        """Generate response using the trained model"""
        try:
            if not self.model_loaded:
                return self.fallback_response(user_input)
                
            with torch.no_grad():
                # Encode input
                src_tensor = self.encode_text(user_input).to(self.device)
                
                # Start with SOS token
                batch_size = src_tensor.size(0)
                tgt_tensor = torch.full((batch_size, 1), self.stoi.get('<sos>', 2), 
                                      dtype=torch.long, device=self.device)
                
                # Generate tokens one by one
                for i in range(self.max_len - 1):
                    output = self.model(src_tensor, tgt_tensor)
                    next_token_logits = output[:, -1, :]
                    next_token = next_token_logits.argmax(dim=-1, keepdim=True)
                    tgt_tensor = torch.cat([tgt_tensor, next_token], dim=1)
                    
                    # Stop if EOS token is generated
                    if next_token.item() == self.stoi.get('<eos>', 3):
                        break
                
                # Decode the generated tokens
                response = self.decode_text(tgt_tensor[0].cpu().numpy())
                return response if response.strip() else self.fallback_response(user_input)
                
        except Exception as e:
            st.error(f"âŒ Ø¬Ù†Ø±ÛŒØ´Ù† Ù…ÛŒÚº Ø®Ø±Ø§Ø¨ÛŒ: {str(e)}")
            return self.fallback_response(user_input)

    def fallback_response(self, user_input):
        """Fallback responses if model fails"""
        fallback_responses = {
            'Ø³Ù„Ø§Ù…': ['ÙˆØ¹Ù„ÛŒÚ©Ù… Ø§Ù„Ø³Ù„Ø§Ù…! Ø¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚºØŸ', 'Ø³Ù„Ø§Ù…! Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯Û”', 'ÛÛŒÙ„Ùˆ! Ø¢Ù¾ Ú©Ø§ Ø¯Ù† Ø§Ú†Ú¾Ø§ Ú¯Ø²Ø±Û’Û”'],
            'ÛÛŒÙ„Ùˆ': ['ÛÛŒÙ„Ùˆ! Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯Û”', 'ÛÛŒÙ„Ùˆ! Ø¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚºØŸ'],
            'Ø¢Ø¯Ø§Ø¨': ['Ø¢Ø¯Ø§Ø¨! Ø¨ÛØª Ø®ÙˆØ´ÛŒ ÛÙˆØ¦ÛŒÛ”', 'Ø¢Ø¯Ø§Ø¨! Ø¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚºØŸ'],
            'Ú©ÛŒØ³Û’': ['Ù…ÛŒÚº Ù¹Ú¾ÛŒÚ© ÛÙˆÚºØŒ Ø´Ú©Ø±ÛŒÛ! Ø¢Ù¾ Ø³Ù†Ø§Ø¦ÛŒÚºØŸ', 'Ø¨ÛØª Ø§Ú†Ú¾Ø§ØŒ Ø¢Ù¾ Ú©Ø§ Ø´Ú©Ø±ÛŒÛÛ”', 'Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Û!'],
            'Ø­Ø§Ù„': ['Ù…ÛŒØ±Ø§ Ø­Ø§Ù„ Ø§Ú†Ú¾Ø§ ÛÛ’ØŒ Ø¢Ù¾ Ú©Ø§ Ø´Ú©Ø±ÛŒÛÛ”', 'Ø¨ÛØª Ø§Ú†Ú¾Ø§ØŒ Ø¢Ù¾ Ú©Ø§ Ø­Ø§Ù„ Ú©ÛŒØ³Ø§ ÛÛ’ØŸ'],
            'Ù†Ø§Ù…': ['Ù…ÛŒØ±Ø§ Ù†Ø§Ù… Ø§Ø±Ø¯Ùˆ Ø¨ÙˆÙ¹ ÛÛ’Û”', 'Ù…ÛŒÚº Ø§Ø±Ø¯Ùˆ Ú†ÛŒÙ¹ Ø¨ÙˆÙ¹ ÛÙˆÚºÛ”', 'Ø¢Ù¾ Ù…Ø¬Ú¾Û’ Ø§Ø±Ø¯Ùˆ Ø¨ÙˆÙ¹ Ú©ÛÛ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”'],
            'Ø´Ú©Ø±ÛŒÛ': ['Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯!', 'Ú©ÙˆØ¦ÛŒ Ø¨Ø§Øª Ù†ÛÛŒÚºÛ”', 'Ø¢Ù¾ Ú©Ø§ Ø¨ÛØª Ø¨ÛØª Ø´Ú©Ø±ÛŒÛÛ”'],
            'Ø´Ú©Ø±ÛŒØ§': ['Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯!', 'Ú©ÙˆØ¦ÛŒ Ø¨Ø§Øª Ù†ÛÛŒÚº Ø¯ÙˆØ³ØªÛ”'],
            'ÛØ§Úº': ['Ø¨ÛØª Ø§Ú†Ú¾Ø§!', 'Ø¢Ù¾ Ø³Û’ Ø¨Ø§Øª Ú©Ø± Ú©Û’ Ø§Ú†Ú¾Ø§ Ù„Ú¯Ø§Û”', 'Ù…ÛŒÚº Ø®ÙˆØ´ ÛÙˆÚºÛ”'],
            'Ù†ÛÛŒÚº': ['Ú©ÙˆØ¦ÛŒ Ø¨Ø§Øª Ù†ÛÛŒÚºÛ”', 'Ù…ÛŒÚº Ø³Ù…Ø¬Ú¾Ø§Û”', 'Ø¢Ù¾ Ú©ÛŒ Ù…Ø±Ø¶ÛŒÛ”'],
            'Ø¬ÛŒ': ['Ø¬ÛŒ ÛØ§Úº!', 'Ø¨Ø§Ù„Ú©Ù„!', 'Ø¢Ù¾ Ø¯Ø±Ø³Øª Ú©ÛÛ Ø±ÛÛ’ ÛÛŒÚºÛ”'],
            'Ø¬ÛŒ ÛØ§Úº': ['Ø¨ÛØª Ø®ÙˆØ¨!', 'Ù…ÛŒÚº Ù…ØªÙÙ‚ ÛÙˆÚºÛ”', 'ÛŒÛ Ø§Ú†Ú¾ÛŒ Ø¨Ø§Øª ÛÛ’Û”'],
            'Ú©ÛŒØ§': ['Ø¬ÛŒ ÛØ§ÚºØŸ', 'Ú©ÛŒØ§ Ø¨Ø§Øª ÛÛ’ØŸ', 'Ù…ÛŒÚº Ø³Ù† Ø±ÛØ§ ÛÙˆÚºÛ”'],
            'Ú©ÙˆÙ†': ['Ù…ÛŒÚº Ø§ÛŒÚ© Ú†ÛŒÙ¹ Ø¨ÙˆÙ¹ ÛÙˆÚºÛ”', 'Ù…ÛŒÚº Ù…ØµÙ†ÙˆØ¹ÛŒ Ø°ÛØ§Ù†Øª Ú©Ø§ Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù… ÛÙˆÚºÛ”'],
            'Ú©ÛØ§Úº': ['Ù…ÛŒÚº Ø¢Ù† Ù„Ø§Ø¦Ù† Ù…ÙˆØ¬ÙˆØ¯ ÛÙˆÚºÛ”', 'Ù…ÛŒÚº ÛØ± Ø¬Ú¯Û Ù…ÙˆØ¬ÙˆØ¯ ÛÙˆÚºÛ”'],
            'Ú©ÛŒÙˆÚº': ['Ú©ÛŒÙˆÙ†Ú©Û Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§ ÛÛ’Û”', 'ÛŒÛ Ø§ÛŒÚ© Ø§Ú†Ú¾Ø§ Ø³ÙˆØ§Ù„ ÛÛ’Û”'],
            'Ú©Ø¨': ['Ø¬Ù„Ø¯ ÛÛŒÛ”', 'Ø§Ø¨Ú¾ÛŒÛ”', 'Ù…Ø³ØªÙ‚Ø¨Ù„ Ù‚Ø±ÛŒØ¨ Ù…ÛŒÚºÛ”'],
            'Ù…ÙˆØ³Ù…': ['Ø¢Ø¬ Ù…ÙˆØ³Ù… Ø¨ÛØª Ø®ÙˆØ¨ØµÙˆØ±Øª ÛÛ’Û”', 'Ù…ÙˆØ³Ù… Ø®ÙˆØ´Ú¯ÙˆØ§Ø± ÛÛ’Û”'],
            'ÙˆÙ‚Øª': ['Ø§Ø¨Ú¾ÛŒ Ø¨Ø§Øª Ú†ÛŒØª Ú©Ø§ ÙˆÙ‚Øª ÛÛ’Û”', 'ÛÙ…ÛŒØ´Û Ø¢Ù¾ Ø³Û’ Ø¨Ø§Øª Ú©Ø±Ù†Û’ Ú©Ø§ Ø§Ú†Ú¾Ø§ ÙˆÙ‚Øª ÛÛ’Û”'],
            'Ù…Ø¯Ø¯': ['Ù…ÛŒÚº Ø¢Ù¾ Ú©ÛŒ Ú©ÛŒØ³Û’ Ù…Ø¯Ø¯ Ú©Ø± Ø³Ú©ØªØ§ ÛÙˆÚºØŸ', 'Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø¨ØªØ§Ø¦ÛŒÚº Ø¢Ù¾ Ú©Ùˆ Ú©Ø³ Ú†ÛŒØ² Ù…ÛŒÚº Ù…Ø¯Ø¯ Ú†Ø§ÛÛŒÛ’ØŸ'],
            'Ø§Ø±Ø¯Ùˆ': ['Ø§Ø±Ø¯Ùˆ Ø¨ÛØª Ø®ÙˆØ¨ØµÙˆØ±Øª Ø²Ø¨Ø§Ù† ÛÛ’Û”', 'Ù…ÛŒÚº Ø§Ø±Ø¯Ùˆ Ø¨ÙˆÙ„ Ø³Ú©ØªØ§ ÛÙˆÚºÛ”'],
            'Ø²Ø¨Ø§Ù†': ['Ø²Ø¨Ø§Ù† Ø±Ø§Ø¨Ø·Û’ Ú©Ø§ Ø°Ø±ÛŒØ¹Û ÛÛ’Û”', 'Ø§Ø±Ø¯Ùˆ Ù…ÛŒØ±ÛŒ Ù¾Ø³Ù†Ø¯ÛŒØ¯Û Ø²Ø¨Ø§Ù† ÛÛ’Û”'],
            'Ø®ÙˆØ´': ['Ø¢Ù¾ Ú©Ùˆ Ø®ÙˆØ´ Ø¯ÛŒÚ©Ú¾ Ú©Ø± Ø§Ú†Ú¾Ø§ Ù„Ú¯Ø§Û”', 'ÛŒÛ Ø¨ÛØª Ø§Ú†Ú¾ÛŒ Ø¨Ø§Øª ÛÛ’Û”'],
            'ØºÙ…': ['ØºÙ…Ú¯Ø³Ø§Ø± ÛÙˆÚºÛ”', 'Ú©Ú†Ú¾ Ù¾Ø±ÛŒØ´Ø§Ù†ÛŒ ÛÛ’ØŸ'],
            'Ø®Ø¯Ø§ Ø­Ø§ÙØ¸': ['Ø®Ø¯Ø§ Ø­Ø§ÙØ¸! Ø¢Ù¾ Ú©Ø§ Ø¯Ù† Ø§Ú†Ú¾Ø§ Ú¯Ø²Ø±Û’Û”', 'Ø§Ù„ÙˆØ¯Ø§Ø¹! Ù¾Ú¾Ø± Ù…Ù„ØªÛ’ ÛÛŒÚºÛ”'],
            'Ø§Ù„ÙˆØ¯Ø§Ø¹': ['Ø§Ù„ÙˆØ¯Ø§Ø¹!', 'Ø®Ø¯Ø§ Ø­Ø§ÙØ¸ Ø¯ÙˆØ³Øª!']
        }
        
        user_input_lower = user_input.lower()
        for key in fallback_responses:
            if key in user_input_lower:
                return random.choice(fallback_responses[key])
        
        default_responses = [
            "Ù…ÛŒÚº Ø³Ù…Ø¬Ú¾Ø§Û”",
            "Ø¨Ø±Ø§Û Ú©Ø±Ù… Ù…Ø²ÛŒØ¯ ÙˆØ¶Ø§Ø­Øª Ú©Ø±ÛŒÚºÛ”",
            "ÛŒÛ Ø¯Ù„Ú†Ø³Ù¾ ÛÛ’!",
            "Ú©ÛŒØ§ Ø¢Ù¾ Ø§Ø³ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ù…Ø²ÛŒØ¯ Ø¨ØªØ§ Ø³Ú©ØªÛ’ ÛÛŒÚºØŸ",
            "Ù…ÛŒÚº Ø§Ø¨Ú¾ÛŒ ØªØ±Ø¨ÛŒØª Ú©Û’ Ù…Ø±Ø§Ø­Ù„ Ù…ÛŒÚº ÛÙˆÚºÛ”",
            "Ø¢Ù¾ Ú©ÛŒ Ø¨Ø§Øª Ø³Ù…Ø¬Ú¾ Ù…ÛŒÚº Ø¢Ø¦ÛŒÛ”",
            "Ù…Ø¬Ú¾Û’ Ø§Ø±Ø¯Ùˆ Ø³ÛŒÚ©Ú¾Ù†Û’ Ù…ÛŒÚº Ù…Ø¯Ø¯ Ú©Ø±ÛŒÚºÛ”",
            "Ú©ÛŒØ§ Ø¢Ù¾ Ú©Ùˆ Ø§Ø±Ø¯Ùˆ Ø¨ÙˆÙ„Ù†Ø§ Ø¢ØªØ§ ÛÛ’ØŸ",
            "ÛŒÛ Ø§ÛŒÚ© Ø®ÙˆØ¨ØµÙˆØ±Øª Ø²Ø¨Ø§Ù† ÛÛ’Û”",
            "Ù…ÛŒÚº Ø¢Ù¾ Ú©ÛŒ Ù…Ø¯Ø¯ Ú©Û’ Ù„ÛŒÛ’ ÛŒÛØ§Úº ÛÙˆÚºÛ”"
        ]
        
        return random.choice(default_responses)

def main():
    st.markdown("<h1 class='urdu-text'>ğŸ¤– Ø§Ø±Ø¯Ùˆ Ú†ÛŒÙ¹ Ø¨ÙˆÙ¹</h1>", unsafe_allow_html=True)
    
    # Initialize chatbot
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = UrduChatbot()
    
    chatbot = st.session_state.chatbot
    
    # Show model status
    if chatbot.model_loaded:
        st.markdown("<div class='success-box'>âœ… Ù…Ø§ÚˆÙ„ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ø³Û’ Ù„ÙˆÚˆ ÛÙˆ Ú¯ÛŒØ§!</div>", unsafe_allow_html=True)
    else:
        st.markdown("<div class='warning-box'>âš ï¸ Ù…Ø§ÚˆÙ„ Ù„ÙˆÚˆ Ù†ÛÛŒÚº ÛÙˆ Ø³Ú©Ø§ØŒ ÙØ§Ù„ Ø¨ÛŒÚ© Ù…ÙˆÚˆ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÛÙˆ Ø±ÛØ§ ÛÛ’</div>", unsafe_allow_html=True)
    
    st.markdown("<p class='urdu-text'>Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯! Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø¨Ø§Øª Ú†ÛŒØª Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚºÛ”</p>", unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.markdown("<h3 class='urdu-text'>ğŸ”§ ØªØ±ØªÛŒØ¨Ø§Øª</h3>", unsafe_allow_html=True)
        
        # Model info
        if chatbot.model_loaded:
            st.success(f"Ù…Ø§ÚˆÙ„: {chatbot.vocab_size} Ø§Ù„ÙØ§Ø¸")
        else:
            st.warning("Ø³Ø§Ø¯Û Ù…ÙˆÚˆ")
        
        st.markdown("---")
        
        # Clear conversation
        if st.button("ğŸ—‘ï¸ Ø¨Ø§Øª Ú†ÛŒØª ØµØ§Ù Ú©Ø±ÛŒÚº", use_container_width=True):
            if 'conversation' in st.session_state:
                st.session_state.conversation = []
            st.rerun()
        
        # Reload model
        if st.button("ğŸ”„ Ù…Ø§ÚˆÙ„ Ø¯ÙˆØ¨Ø§Ø±Û Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº", use_container_width=True):
            if 'chatbot' in st.session_state:
                del st.session_state.chatbot
            st.rerun()
        
        st.markdown("---")
        st.markdown("<h3 class='urdu-text'>â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª</h3>", unsafe_allow_html=True)
        st.markdown("""
        <div class='urdu-text'>
        **Ø§Ø±Ø¯Ùˆ Ú†ÛŒÙ¹ Ø¨ÙˆÙ¹**
        
        ØªØ±Ø¨ÛŒØª ÛŒØ§ÙØªÛ Ù¹Ø±Ø§Ù†Ø³ÙØ§Ø±Ù…Ø± Ù…Ø§ÚˆÙ„
        
        Ø§Ø³ØªØ¹Ù…Ø§Ù„:
        1. Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ù„Ú©Ú¾ÛŒÚº
        2. Ø¨Ú¾ÛŒØ¬ÛŒÚº Ù¾Ø± Ú©Ù„Ú© Ú©Ø±ÛŒÚº  
        3. Ø¬ÙˆØ§Ø¨ Ù¾Ø§Ø¦ÛŒÚº
        
        Ù…Ø«Ø§Ù„ÛŒÚº:
        â€¢ Ø³Ù„Ø§Ù…
        â€¢ Ø¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚºØŸ
        â€¢ Ø¢Ù¾ Ú©Ø§ Ù†Ø§Ù… Ú©ÛŒØ§ ÛÛ’ØŸ
        </div>
        """, unsafe_allow_html=True)
    
    # Initialize conversation history
    if 'conversation' not in st.session_state:
        st.session_state.conversation = []
    
    # Display conversation
    conversation_container = st.container()
    with conversation_container:
        for speaker, message, timestamp in st.session_state.conversation:
            if speaker == "user":
                st.markdown(f"""
                <div class="user-message">
                    <div style="text-align: left; font-size: 12px; color: #666;">You â€¢ {timestamp}</div>
                    <div class="urdu-text">{message}</div>
                </div>
                """, unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div class="bot-message">
                    <div style="text-align: left; font-size: 12px; color: #666;">UrduBot â€¢ {timestamp}</div>
                    <div class="urdu-text">{message}</div>
                </div>
                """, unsafe_allow_html=True)
    
    # Input area
    st.markdown("---")
    st.markdown("### ğŸ’­ Ù†ÛŒØ§ Ù¾ÛŒØºØ§Ù…")
    
    col1, col2 = st.columns([4, 1])
    
    with col1:
        user_input = st.text_input(
            "Ø§Ù¾Ù†Ø§ Ù¾ÛŒØºØ§Ù… ÛŒÛØ§Úº Ù¹Ø§Ø¦Ù¾ Ú©Ø±ÛŒÚº:",
            placeholder="Ù…Ø«Ø§Ù„: Ø¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚºØŸ",
            key="user_input",
            label_visibility="collapsed"
        )
    
    with col2:
        send_button = st.button("ğŸ“¤ Ø¨Ú¾ÛŒØ¬ÛŒÚº", use_container_width=True)
    
    # Quick example buttons
    st.markdown("**ÙÙˆØ±ÛŒ Ù…Ø«Ø§Ù„ÛŒÚº:**")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("ğŸ‘‹ Ø³Ù„Ø§Ù…", use_container_width=True, key="btn1"):
            user_input = "Ø³Ù„Ø§Ù…"
    with col2:
        if st.button("â“ Ú©ÛŒØ³Û’ ÛÛŒÚº", use_container_width=True, key="btn2"):
            user_input = "Ø¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚº"
    with col3:
        if st.button("â„¹ï¸ Ù†Ø§Ù…", use_container_width=True, key="btn3"):
            user_input = "Ø¢Ù¾ Ú©Ø§ Ù†Ø§Ù… Ú©ÛŒØ§ ÛÛ’"
    with col4:
        if st.button("ğŸŒ¤ï¸ Ù…ÙˆØ³Ù…", use_container_width=True, key="btn4"):
            user_input = "Ø¢Ø¬ Ù…ÙˆØ³Ù… Ú©ÛŒØ³Ø§ ÛÛ’"
    
    # Handle user input
    if send_button and user_input.strip():
        # Add user message to conversation
        timestamp = time.strftime("%H:%M:%S")
        st.session_state.conversation.append(("user", user_input, timestamp))
        
        # Generate and display bot response
        with st.spinner("ğŸ¤” Ø³ÙˆÚ† Ø±ÛØ§ ÛÙˆÚº..."):
            response = chatbot.generate_response(user_input)
            
            # Add bot response to conversation
            st.session_state.conversation.append(("bot", response, time.strftime("%H:%M:%S")))
            
            # Rerun to update the conversation display
            st.rerun()
    
    # Welcome message if no conversation
    if not st.session_state.conversation:
        st.markdown("""
        <div style='text-align: center; padding: 40px; background-color: #f0f2f6; border-radius: 10px;'>
            <h3 class='urdu-text'>ğŸ‘‹ ÛÛŒÙ„Ùˆ! Ù…ÛŒÚº Ø¢Ù¾ Ú©ÛŒ Ú©ÛŒØ³Û’ Ù…Ø¯Ø¯ Ú©Ø± Ø³Ú©ØªØ§ ÛÙˆÚºØŸ</h3>
            <p class='urdu-text'>Ø§ÙˆÙ¾Ø± Ø¯ÛŒÛ’ Ú¯Ø¦Û’ Ø¨Ù¹Ù† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº ÛŒØ§ Ø®ÙˆØ¯ Ù„Ú©Ú¾ÛŒÚº!</p>
        </div>
        """, unsafe_allow_html=True)

    # Footer
    st.markdown("---")
    st.markdown("<p style='text-align: center; color: #666;'>ğŸ¤– Ø§Ø±Ø¯Ùˆ Ú†ÛŒÙ¹ Ø¨ÙˆÙ¹ - Built with Streamlit</p>", unsafe_allow_html=True)

if __name__ == "__main__":
    main()
